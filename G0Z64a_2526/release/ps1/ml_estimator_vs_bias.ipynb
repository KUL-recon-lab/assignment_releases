{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name, r-number, collaborators, and the generative AI statement below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in your name, r-number, and collaborators (if any)\n",
    "NAME = \"\"\n",
    "r_number=\"\"\n",
    "COLLABORATORS = \"\"\n",
    "\n",
    "# gen-AI (LLM) statement. In case large language models (generative AI systems such as chatGPT, co-pilot, ...) were used to complete this notebook, \n",
    "# fill in which models were used and what for\n",
    "gen_AI_statement = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e58175494fe89abc134d4ef124849c0f",
     "grade": false,
     "grade_id": "cell-0a1b8b466dce098f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import modules needed for this notebook\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, Circle\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "print(f\"Numpy version      : {np.__version__}\")\n",
    "print(f\"Matplotlib version : {matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f0a84bfc0d08b78cbdc1e1f973c718ed",
     "grade": false,
     "grade_id": "cell-7043199da2909d8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## Motivation & Aim\n",
    "\n",
    "In nuclear medicine physics, many reconstruction and quantification problems ultimately reduce to **estimating unknown parameters from noisy counting data**. Because the measurements are counts, the natural statistical model is the **Poisson distribution**, and one of the most widely used estimation strategies is the **maximum likelihood estimator (MLE)**.  \n",
    "\n",
    "The concept of MLE is e.g. used when we reconstruct PET images using the iteratice MLEM algorithm. \n",
    "MLEM converges to the MLE (maximum likelihood image) given a set of count measurements along different line of response as shown below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff05b193b48774f314a6fcddf53498ee",
     "grade": false,
     "grade_id": "cell-2f33cd8cc1a9db05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "figp, axp = plt.subplots(figsize=(6, 6))\n",
    "axp.set_aspect('equal')\n",
    "axp.axis('off')\n",
    "\n",
    "# --- Parameters ---\n",
    "n = 4\n",
    "cell = 1.0\n",
    "half = n * cell / 2\n",
    "R_min = (2**0.5) * half\n",
    "R = R_min + 0.35\n",
    "N_det = 36\n",
    "theta0 = 0.0\n",
    "\n",
    "# --- Pixel grid ---\n",
    "x0, y0 = -half, -half\n",
    "k = 1\n",
    "for j in range(n):\n",
    "    for i in range(n):\n",
    "        axp.add_patch(Rectangle((x0 + i*cell, y0 + j*cell), cell, cell,\n",
    "                                fill=False, linewidth=1.5, edgecolor='black'))\n",
    "        cx = x0 + (i + 0.5) * cell\n",
    "        cy = y0 + (j + 0.5) * cell\n",
    "        axp.text(cx, cy, rf'$x_{{{k}}}$', ha='center', va='center', fontsize=11)\n",
    "        k += 1\n",
    "\n",
    "# Outer border\n",
    "axp.add_patch(Rectangle((x0, y0), n*cell, n*cell,\n",
    "                        fill=False, linewidth=1.8, edgecolor='black'))\n",
    "\n",
    "# --- Detector ring ---\n",
    "axp.add_patch(Circle((0, 0), R, fill=False, linewidth=3, edgecolor='black'))\n",
    "angles = theta0 + 2*np.pi*np.arange(N_det)/N_det\n",
    "xs = R * np.cos(angles)\n",
    "ys = R * np.sin(angles)\n",
    "axp.scatter(xs, ys, s=28, color='#f39c12', zorder=3)\n",
    "\n",
    "# Label every 4th detector\n",
    "for m in range(0, N_det, 4):\n",
    "    axp.text((R + 0.18) * np.cos(angles[m]),\n",
    "             (R + 0.18) * np.sin(angles[m]),\n",
    "             f'$d_{{{m}}}$', ha='center', va='center', fontsize=10)\n",
    "\n",
    "# --- Fixed LOR pairs ---\n",
    "pairs = [(10, 22), (2, 18), (13, 27), (6, 30)]\n",
    "labels = [r'$y_1$', r'$y_2$', r'$y_3$', r'$y_4$']\n",
    "lor_color = cm.tab10(1)\n",
    "\n",
    "for (p, q), lab in zip(pairs, labels):\n",
    "    x1, y1 = xs[p], ys[p]\n",
    "    x2, y2 = xs[q], ys[q]\n",
    "    axp.plot([x1, x2], [y1, y2], linewidth=1.8, color=lor_color, zorder=2)\n",
    "\n",
    "\n",
    "axp.set_xlim(-R - 0.6, R + 0.6)\n",
    "axp.set_ylim(-R - 0.6, R + 0.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20b24dc58096aee43a3e6fe486e5b431",
     "grade": false,
     "grade_id": "cell-0adec9917cd8408a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "An important property of any estimator is whether it is **biased**: does its expected value equal the true underlying parameter, or does it systematically over- or under-estimate? Understanding bias (related accuracy) is crucial, systematic bias can lead to incorrect conclusions.\n",
    "A 2nd important propert is **variance** (related called precision). \n",
    "Ideally, we want an unbias estimator that has low variance.\n",
    "\n",
    "In real PET reconstructions, where we reconstruct millions of voxels from count values along billions of LORs, finding the MLE requires iterative algorithms (e.g. MLEM).\n",
    "\n",
    "That is why, in this notebook, we explore a **simplified toy forward model** consisting of 2 voxels and 3 LORs as shown below that captures the essence of more complex problems in emission tomography:\n",
    "\n",
    "$$\n",
    "\\bar y_1 (x_1, x_2) = \\alpha x_1,\\qquad \n",
    "\\bar y_2 (x_1, x_2) = \\alpha x_2,\\qquad \n",
    "\\bar y_3 (x_1, x_2) = \\alpha (x_1+x_2),\n",
    "$$\n",
    "\n",
    "where $\\bar y_i$ are the expectations of **Poisson-distributed**  observed counts, $\\alpha$ is a known system constant modeling acquisition time and detector sensitivity, and $x_1, x_2$ are the unknown parameters (pixel values) to be estimated.\n",
    "\n",
    "This simplified toy model has the advantage that we can derive an analytical expression for the MLE avoiding lengthy iterative algorithms and is shown below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d5ea9f927ba0e852ed007684463669a",
     "grade": false,
     "grade_id": "cell-0da908de38b92db8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fige, axe = plt.subplots(figsize=(7, 4), layout='constrained')\n",
    "\n",
    "voxel_size = 1.0\n",
    "axe.add_patch(Rectangle((0, 0), voxel_size, voxel_size, fill=False, linewidth=2))\n",
    "axe.text(0.2, 0.1, r\"$x_1$\", ha=\"center\", va=\"center\", fontsize=\"large\")\n",
    "axe.add_patch(Rectangle((voxel_size, 0), voxel_size, voxel_size, fill=False, linewidth=2))\n",
    "axe.text(1.2, 0.1, r\"$x_2$\", ha=\"center\", va=\"center\", fontsize=\"large\")\n",
    "\n",
    "# LOR 1: vertical through voxel 1\n",
    "axe.plot([0.5, 0.5], [-0.3, 1.3], linewidth=1.5)\n",
    "axe.text(0.05, 1.35, r\"$y_1 \\sim \\mathrm{Poisson}(\\alpha x_1)$\", va=\"bottom\", color = plt.cm.tab10(0))\n",
    "\n",
    "# LOR 2: vertical through voxel 2\n",
    "axe.plot([1.5, 1.5], [-0.3, 1.3], linewidth=1.5)\n",
    "axe.text(1.55, 1.35, r\"$y_2 \\sim \\mathrm{Poisson}(\\alpha x_2)$\", va=\"bottom\", color = plt.cm.tab10(1))\n",
    "\n",
    "# LOR 3: horizontal through center of voxels (y=0.5)\n",
    "axe.plot([-0.6, 2.6], [0.5, 0.5], linewidth=1.5, color=\"green\")\n",
    "axe.text(2.65, 0.55, r\"$y_3 \\sim \\mathrm{Poisson}(\\alpha(x_1+x_2))$\", va=\"bottom\", ha=\"left\", color = plt.cm.tab10(2))\n",
    "\n",
    "axe.set_aspect('equal')\n",
    "axe.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da20463f7d1962c35524ba864d02aa3b",
     "grade": false,
     "grade_id": "cell-02e6584a9777252f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This assignment has two main goals:\n",
    "\n",
    "1. **Analytical understanding**: Derive the closed-form MLE for $(x_1, x_2)$. This will give you practice in setting up and solving likelihood equations â€” an important skill that is useful when estimating parameters from measured data applicable in many different scientific fields.\n",
    "2. **Computational experiment**: Use **Monte Carlo simulations** to test whether a given (non-trivial) estimator is biased. You will simulate many Poisson datasets (noise realizations of measurements), compute the estimates, and study whether their average matches the true parameters. Moreover, you will also estimate the variance of the estimator.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "253ada99fc7e42285062f8670b4bbbea",
     "grade": false,
     "grade_id": "cell-d31c25ad61831246",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Least squares estimator (LSE)\n",
    "\n",
    "One possible (but maybe not optimal) estimator is the **least squares estimator (LSE)**, which minimizes the squared error ($SE$) between the observed ($y_i$) and expected counts ($\\bar y_i$) given by\n",
    "$$\n",
    "SE(x_1, x_2) = \\sum_{i=1}^3 (\\bar y_i(x_1, x_2) - y_i)^2 \\quad .\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\hat x_1^{LSE}, \\hat x_2^{LSE}) = \\arg\\min_{x_1, x_2} SE(x_1, x_2).\n",
    "$$\n",
    "\n",
    "For this toy model, the LSE has the simple closed-form expressions\n",
    "$$\n",
    "\\hat x_1^{LSE} = \\frac{2y_1 + (y_3 - y_2)}{3\\alpha},\\qquad\n",
    "\\hat x_2^{LSE} = \\frac{2y_2 + (y_3 - y_1)}{3\\alpha} \\ ,\n",
    "$$\n",
    "\n",
    "which we implement in two functions in the next code cell.\n",
    "We will compare the properties of LSE against the MLE you derive in the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac3812e72649462386f533ec5ff6d7ce",
     "grade": false,
     "grade_id": "cell-1d0c26448ff821a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def lse_x1(alpha: float, y1: int, y2: int, y3: int) -> float:\n",
    "    \"\"\"Compute the least squares estimator for x1, \n",
    "    given the forward model above, the measurements y1, y2, y3 and system sensitivity alpha.\n",
    "    \"\"\"\n",
    "    return (2*y1 + (y3 - y2))/(3*alpha)\n",
    "\n",
    "def lse_x2(alpha: float, y1: int, y2: int, y3: int) -> float:\n",
    "    \"\"\"Compute the least squares estimator for x2, \n",
    "    \"\"\"\n",
    "    return (2*y2 + (y3 - y1))/(3*alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88158572c7a72b71753b6dd167edcff9",
     "grade": false,
     "grade_id": "cell-4bb560cdac9dc7cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 1 - Maximum Likelihood Estimator (MLE) of the Poisson log-likelihood\n",
    "\n",
    "Derive a closed-form expression for the maximum likelihood estimator (MLE) for $(x_1, x_2)$ as a function of $\\alpha$, $y_1$, $y_2$, and $y_3$. \n",
    "The MLE is defined as the $(x_1, x_2)$ values that maximize the Poisson log-likelihood function:\n",
    "$$\n",
    "\\log L(x_1, x_2) = \\sum_{i=1}^3 \\left( y_i \\ln(\\bar y_i(x_1, x_2)) - \\bar y_i(x_1, x_2) - \\ln y_i! \\right).\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\hat x_1^{MLE}, \\hat x_2^{MLE}) = \\arg\\max_{x_1, x_2} \\log L(x_1, x_2).\n",
    "$$\n",
    "\n",
    "Implement both expressions in the python functions below. **Make sure that both functions also return meaningful values for the special case** $y_1 = 0$ and $y_2 = 0$. *Note: in the latter special case the MLE is not unique, choose it such that in this case, the MLE for $x_1$ and $x_2$ are the same.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "901aa46ead2e62b7531a38cee23681b5",
     "grade": false,
     "grade_id": "mle_x1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mle_x1(alpha: float, y1: int, y2: int, y3:int) -> float:\n",
    "    \"\"\"Compute the maximum likelihood estimator for x1, \n",
    "    given the forward model above assuming Poisson distributed measurements y1, y2, y3 and system sensitivity alpha.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f41a08d3065491c98dd9c5532d64fee",
     "grade": true,
     "grade_id": "mle_x1_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# print the output of the MLE for x1 for different inputs\n",
    "for a, b, c, d in ((0.5, 1, 3, 2), (2.5, 5, 7, 9), (1.5, 6, 2, 7), (0.5, 0, 0, 2)):\n",
    "    print(f\"alpha = {a}, y1 = {b}, y2 = {c}, y3 = {d}: MLE for x1 = {mle_x1(alpha = a, y1 = b, y2 = c, y3 = d)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31c97229c5988089d2c445683afaff5b",
     "grade": false,
     "grade_id": "mle_x2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mle_x2(alpha: float, y1: int, y2: int, y3:int) -> float:\n",
    "    \"\"\"Compute the maximum likelihood estimator for x2, \n",
    "    given the forward model above assuming Poisson distributed measurements y1, y2, y3 and system sensitivity alpha.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0348b1ff600a22d67b97202c42defa4",
     "grade": true,
     "grade_id": "mle_x2_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# print the output of the MLE for x2 for different inputs\n",
    "for a, b, c, d in ((0.5, 1, 3, 2), (2.5, 5, 7, 9), (1.5, 6, 2, 7), (0.5, 0, 0, 2)):\n",
    "    print(f\"alpha = {a}, y1 = {b}, y2 = {c}, y3 = {d}: MLE for x2 = {mle_x2(alpha = a, y1 = b, y2 = c, y3 = d)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d9d65fd0f792b7d0a85a00b1a3371bb",
     "grade": false,
     "grade_id": "cell-3e839e845fe6894b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 2 - Monte Carlso-based bias and variance estimation\n",
    "\n",
    "Analytically calculating the bias and variance of the MLE is quite involved. (Why?) \n",
    "\n",
    "Numerically, we can estimate the bias and variance of any estimator using Monte Carlo simulations. The idea is as follows:\n",
    "1. Choose a set of true parameters $(x_1^{\\text{true}}, x_2^{\\text{true}})$ and a system constant $\\alpha$.\n",
    "2. Compute the corresponding noise-free mean measurements $\\bar y_i = \\bar y_i(x_1^{\\text{true}}, x_2^{\\text{true}})$.\n",
    "3. Simulate $N$ independent noise realizations of the measurements $y_i^{(j)} \\sim \\text{Poisson}(\\bar y_i)$ for $j = 1, \\ldots, N$.\n",
    "4. For each noise realization, compute the estimates $(\\hat x_1^{(j)}, \\hat x_2^{(j)})$ using a given estimator (e.g., the MLE).\n",
    "5. Estimate the bias and variance of the estimator using the sample mean and sample variance of the estimates:\n",
    "   - Bias: $\\text{Bias}(\\hat x_i) \\approx \\frac{1}{N} \\sum_{j=1}^N \\hat x_i^{(j)} - x_i^{\\text{true}}$ - hint: use `np.mean()` for the first term\n",
    "   - Variance: $\\text{Var}(\\hat x_i) \\approx \\frac{1}{N-1} \\sum_{j=1}^N (\\hat x_i^{(j)} - \\bar{\\hat x_i})^2$, where $\\bar{\\hat x_i} = \\frac{1}{N} \\sum_{j=1}^N \\hat x_i^{(j)}$ - hint: use `np.var(ddof = 1)`\n",
    "\n",
    "*Note: For demonstration purposes (and short runtimes), we choose `N=int(1e6)` noise realizations below. Increasesing this value will yield more correct results.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61a3ee5eb2c366d795b2925c11941d0c",
     "grade": false,
     "grade_id": "cell-efb38a583c1b1a6e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# step 1: choose \"true\" values for alpha, x1, x2\n",
    "alpha_test: float = 0.5\n",
    "x1_true: float = 1.8\n",
    "x2_true: float = 1.0\n",
    "\n",
    "# step 2: compute the corresponding noise-free measurements\n",
    "ybar_1 = alpha_test*x1_true\n",
    "ybar_2 = alpha_test*x2_true\n",
    "ybar_3 = alpha_test*(x1_true + x2_true)\n",
    "\n",
    "\n",
    "# step 3: simulate N noisy measurements from the Poisson distribution\n",
    "N = int(1e6)\n",
    "\n",
    "y1_test = np.random.poisson(ybar_1, N)\n",
    "y2_test = np.random.poisson(ybar_2, N)\n",
    "y3_test = np.random.poisson(ybar_3, N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b8c0aaa3350fc05980df65fce3ad0e7",
     "grade": false,
     "grade_id": "cell-3ed88e5ba2c4c20f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "First, we apply the Monte Carlo procedure to the LSE to numerically estimate the bias and variance of the LSE for the given true parameters and system constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ea5781c3a34c5d8411dd6700e70d398",
     "grade": false,
     "grade_id": "cell-828668d0dacb1d47",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# step 4: compute the estimators for each noisy measurement\n",
    "#         here we use the above defined (naive) least squares estimator (LSE)\n",
    "\n",
    "x1_lse = np.zeros(N)\n",
    "x2_lse = np.zeros(N)\n",
    "\n",
    "for i in range(N):\n",
    "    x1_lse[i] = lse_x1(alpha_test, y1_test[i], y2_test[i], y3_test[i])\n",
    "    x2_lse[i] = lse_x2(alpha_test, y1_test[i], y2_test[i], y3_test[i])\n",
    "\n",
    "print(f\"numeric expectation of LSEs - x1: {x1_lse.mean(): .5f}, x2: {x2_lse.mean(): .5f}\")\n",
    "print(f\"numeric bias of LSEs        - x1: {(x1_lse.mean() - x1_true): .5f}, x2: {(x2_lse.mean() - x2_true): .5f}\")\n",
    "print(f\"numeric variances of LSEs   - x1: {x1_lse.var(ddof=1): .5f}, x2: {x2_lse.var(ddof=1): .5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0f66c76cf1dc93a203f0224f419d660",
     "grade": false,
     "grade_id": "cell-f8a79d35acb1708e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before doing the same for the above implemented MLE, we will visualize the distribution of LSEs using histograms and box plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8556c2b7a8c070550fa7481913bc2242",
     "grade": false,
     "grade_id": "cell-6c91046a791e0792",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "bowplot_kws = dict(\n",
    "    positions=[1],\n",
    "    widths=0.3,\n",
    "    vert=False,\n",
    "    showfliers=False,\n",
    "    showmeans=True,\n",
    "    meanline=True,\n",
    "    medianprops=dict(linewidth=0),\n",
    "    meanprops=dict(color='black', linestyle='-', linewidth=1.5))\n",
    "\n",
    "fig1, ax1 = plt.subplots(\n",
    "    2, 1, figsize=(8, 3), layout='constrained', sharex=True, sharey=True\n",
    ")\n",
    "\n",
    "\n",
    "# Bottom axis: histograms\n",
    "bins = np.linspace(min(x1_lse.min(), x2_lse.min()), 1.01*max(x1_lse.max(), x2_lse.max()), 200)\n",
    "\n",
    "ax1[0].hist(x1_lse, bins=bins, label='x1_lse', density=True, color ='C0')\n",
    "ax1[0].axvline(x1_true, color='red', ls=':', lw = 1.0, label='true x1')\n",
    "ax1[0].boxplot(x1_lse, **bowplot_kws)\n",
    "ax1[1].hist(x2_lse, bins=bins, label='x2_lse', density=True, color ='C1')\n",
    "ax1[1].axvline(x2_true, color='red', ls=':', lw = 1.0, label='true x2')\n",
    "ax1[1].boxplot(x2_lse, **bowplot_kws)\n",
    "ax1[0].legend()\n",
    "ax1[1].legend()\n",
    "\n",
    "\n",
    "ax1[1].set_xlabel('Estimator Value')\n",
    "ax1[0].set_title('Histograms and boxplots of x1_lse and x2_lse', fontsize = \"medium\")\n",
    "ax1[0].grid(ls=\":\")\n",
    "ax1[1].grid(ls=\":\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1bf1b6495cbdd4f9577c13b8c95ca39a",
     "grade": false,
     "grade_id": "cell-1beb7bcdfe4ee4a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we repeat the Monte Carlo procedure for the MLE and compare the bias and variance of the MLE against the LSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "562300d7aa35dfa5f3f677d660a1c36b",
     "grade": false,
     "grade_id": "cell-098dc9feb8258552",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# step 4: compute the estimators for each noisy measurement\n",
    "#         here we use the above defined maximum likelihood estimator (MLE)\n",
    "\n",
    "x1_mle = np.zeros(N)\n",
    "x2_mle = np.zeros(N)\n",
    "\n",
    "for i in range(N):\n",
    "    x1_mle[i] = mle_x1(alpha_test, y1_test[i], y2_test[i], y3_test[i])\n",
    "    x2_mle[i] = mle_x2(alpha_test, y1_test[i], y2_test[i], y3_test[i])\n",
    "\n",
    "print(f\"numeric expectations of MLEs - x1: {x1_mle.mean(): .5f}, x2: {x2_mle.mean(): .5f}\")\n",
    "print(f\"numeric bias of MSEs         - x1: {(x1_mle.mean() - x1_true): .5f}, x2: {(x2_mle.mean() - x2_true): .5f}\")\n",
    "print(f\"numeric variances of MLEs    - x1: {x1_mle.var(ddof=1): .5f}, x2: {x2_mle.var(ddof=1): .5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "774bd5ea36337dcff81b1bba3c5bf376",
     "grade": false,
     "grade_id": "cell-b19c89889cd4ff0b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We also visualize the distribution of MLEs using histograms and box plots and compare them against the LSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b702123eee7e49d0f22d93b5e2c4027",
     "grade": false,
     "grade_id": "cell-63c5d0986ebdc767",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig2, ax2 = plt.subplots(4, 1, figsize=(8, 6), layout='constrained', sharex=True, sharey=False)\n",
    "\n",
    "# x1_lse\n",
    "ax2[0].hist(x1_lse, bins=bins, label='x1_lse', density=True, color='C0')\n",
    "ax2[0].axvline(x1_true, color='red', ls=':', lw=1.0, label='true x1')\n",
    "ax2[0].boxplot(x1_lse, **bowplot_kws)\n",
    "ax2[0].legend()\n",
    "ax2[0].set_title('Histograms and boxplots of x1_lse, x1_mle, x2_lse, and x2_mle', fontsize = \"medium\")\n",
    "\n",
    "# x1_mle\n",
    "ax2[1].hist(x1_mle, bins=bins, label='x1_mle', density=True, color='C2')\n",
    "ax2[1].axvline(x1_true, color='red', ls=':', lw=1.0, label='true x1')\n",
    "ax2[1].boxplot(x1_mle, **bowplot_kws)\n",
    "ax2[1].legend()\n",
    "\n",
    "# x2_lse\n",
    "ax2[2].hist(x2_lse, bins=bins, label='x2_lse', density=True, color='C1')\n",
    "ax2[2].axvline(x2_true, color='red', ls=':', lw=1.0, label='true x2')\n",
    "ax2[2].boxplot(x2_lse, **bowplot_kws)\n",
    "ax2[2].legend()\n",
    "\n",
    "# x2_mle\n",
    "ax2[3].hist(x2_mle, bins=bins, label='x2_mle', density=True, color='C3')\n",
    "ax2[3].axvline(x2_true, color='red', ls=':', lw=1.0, label='true x2')\n",
    "ax2[3].boxplot(x2_mle, **bowplot_kws)\n",
    "ax2[3].legend()\n",
    "\n",
    "ax2[3].set_xlabel('Estimator Value')\n",
    "for ax in ax2:\n",
    "    ax.grid(ls=\":\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "915f1cbdb935beb1257caa8e39d8ab8a",
     "grade": false,
     "grade_id": "cell-225543ea502070f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 3 - Analyzing and interpreting the simulation results\n",
    "\n",
    "Answer the following True or False questions based on your interpretation of the results from this notebook by completing the following functions.\n",
    "\n",
    "*Note: please only implement `return True` or `return False` to indicate whether the statement in the function names for our toy model using the values $\\alpha = 0.5$, $x_1 = 1.8$, $x_2 = 1.0$ is true or false.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd1a001e71a6ab49fd3fd900aa275b9e",
     "grade": false,
     "grade_id": "x1_lse_bias",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def x1_lse_has_more_than_2_percent_bias() -> bool:\n",
    "    \"\"\"Returns True if the relative bias of LSE for x1 is more than 2%, False otherwise.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3db98f019c76fe87c89509eb8bb9d9a2",
     "grade": true,
     "grade_id": "x1_lse_bias_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"x1_lse_has_more_than_2_percent_bias(): {x1_lse_has_more_than_2_percent_bias()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f09ecdc98ef253108e417e278a5f5af",
     "grade": false,
     "grade_id": "x2_lse_bias",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def x2_lse_has_more_than_2_percent_bias() -> bool:\n",
    "    \"\"\"Returns True if the relative bias of LSE for x2 is more than 2%, False otherwise.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d93f2186b4b7b395b8d8a9e0ffcf2452",
     "grade": true,
     "grade_id": "x2_lse_bias_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"x2_lse_has_more_than_2_percent_bias(): {x2_lse_has_more_than_2_percent_bias()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b737d5deafc178e1a55ec861084aba0",
     "grade": false,
     "grade_id": "x1_mle_bias",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def x1_mle_has_more_than_2_percent_bias() -> bool:\n",
    "    \"\"\"Returns True if the relative bias of MLE for x1 is more than 2%, False otherwise.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54bc444d6bdb7adb0c4d71193194c6d4",
     "grade": true,
     "grade_id": "x1_mle_bias_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"x1_mle_has_more_than_2_percent_bias(): {x1_mle_has_more_than_2_percent_bias()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5b95defc55a3e8a77dbd28ed4ef7e99",
     "grade": false,
     "grade_id": "x2_mle_bias",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def x2_mle_has_more_than_2_percent_bias() -> bool:\n",
    "    \"\"\"Returns True if the relative bias of MLE for x2 is more than 2%, False otherwise.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49e94da86d8f87983aa5d89c205d9413",
     "grade": true,
     "grade_id": "x2_mle_bias_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"x2_mle_has_more_than_2_percent_bias(): {x2_mle_has_more_than_2_percent_bias()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b94af82a209a32c8a965c94980c8944",
     "grade": false,
     "grade_id": "var1_comp",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def x1_lse_has_smaller_variance_compared_to_x1_mle() -> bool:\n",
    "    \"\"\"Returns True if the variance of the LSE of x1 is smaller than the variance of the MLE of x1, False otherwise.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f74994402467272838ad670e2959c1c",
     "grade": true,
     "grade_id": "var1_comp_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"x1_lse_has_smaller_variance_compared_to_x1_mle(): {x1_lse_has_smaller_variance_compared_to_x1_mle()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01099e018ef090930f2b93bccaa33b48",
     "grade": false,
     "grade_id": "var2_comp",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def x2_lse_has_smaller_variance_compared_to_x2_mle() -> bool:\n",
    "    \"\"\"Returns True if the variance of the LSE of x2 is smaller thant the variance of the MLE of x2, False otherwise.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c02af1cd997fb85c1a85cd88f885c6c",
     "grade": true,
     "grade_id": "var2_comp_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"x2_lse_has_smaller_variance_compared_to_x2_mle(): {x2_lse_has_smaller_variance_compared_to_x2_mle()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
